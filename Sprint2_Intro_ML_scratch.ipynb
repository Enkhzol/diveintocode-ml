{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sprint2_Intro_ML_scratch.ipynb",
      "provenance": [],
      "mount_file_id": "1P8E0WG2Nxz2x6SC4KM65YnL9fLuQLTG3",
      "authorship_tag": "ABX9TyOmNgrfdJcY49YjnuWmo6vt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Enkhzol/diveintocode-ml/blob/master/Sprint2_Intro_ML_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. About this Sprint​ ​**\n",
        "\n",
        "The purpose of this Sprint\n",
        "\n",
        "    Prepare for machine learning scratch\n",
        "\n",
        "\n",
        "How to learn\n",
        "\n",
        "First, use scikit-learn to implement a machine learning program that will be implemented by scratch in future learning.\n",
        "\n",
        "\n",
        "**2. What is scratch? **\n",
        "\n",
        "By combining the basic libraries provided in NumPy etc., you can create your own classes / functions equivalent to the functions implemented in the applied libraries such as scikit-learn. This is called scratch.​\n",
        "\n",
        "Through scratching, it is difficult to grasp just by moving a library such as scikit-learn, and we aim for a deep understanding of the algorithm. It also improves your coding skills, but that's not the main purpose.\n",
        "\n",
        "We are aiming for the following effects.\n",
        "\n",
        "    Make it easier to understand theory and mathematical formulas when encountering new methods\n",
        "    Reduce ambiguity in using libraries\n",
        "    Make existing implementations easier to read\n",
        "\n",
        "\n",
        "This time, first, we will implement the machine learning program using scikit-learn without completely scratching it. Then, from the next time, we will gradually shift the implementation using scikit-learn to scratch.\n"
      ],
      "metadata": {
        "id": "F-cqZK1ynYUP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[Problem 1] Scratch of train_test_split​**\n",
        "\n",
        "First, try scratching train_test_split of scikit-learn. Please implement the function based on the following template.\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
        "\n",
        "Be sure to check if the created function train_test_split​\n",
        "\n",
        " Prototype\n",
        " "
      ],
      "metadata": {
        "id": "FEsCsx5GnuER"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "def scratch_train_test_split(X, y, train_size=0.8):\n",
        "    \"\"\"Divide the validation data.\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : ndarray\n",
        "      Training data (n_samples, n_features)\n",
        "    y : ndarray\n",
        "      Correct answer value (n_samples,)\n",
        "    train_size : float\n",
        "      Specify what percentage to use as a train (0 < train_size < 1)\n",
        "    Returns\n",
        "    -------\n",
        "    X_train : ndarray\n",
        "      Training data (n_samples, n_features)\n",
        "    X_test : ndarray\n",
        "      Validation data (n_samples, n_features)\n",
        "    y_train : ndarray\n",
        "      Correct answer value of training data (n_samples,)\n",
        "    y_test : ndarray\n",
        "      Correct value of verification data (n_samples,)\n",
        "    \"\"\"\n",
        "    # Write code here\n",
        "    pass\n",
        "    return X_train, X_test, y_train, y_test"
      ],
      "metadata": {
        "id": "1x7FTi5Wn9yd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def scratch_train_test_split(X, y, test_size=0.3, random_state = 0):\n",
        "    n_samples = X.shape[0]\n",
        "\n",
        "    #floor is devaluated \n",
        "    n_train = np.floor((1-test_size) * n_samples).astype(int)\n",
        "    n_test = n_samples - n_train\n",
        "    \n",
        "    rng = np.random.RandomState(seed=random_state)\n",
        "    #n_samples(Randomly arrange 100 rows of X (0 to 99))\n",
        "    permutation = rng.permutation(n_samples)\n",
        "    \n",
        "    #Randomly arranged n_samples are selected from the top of n_test\n",
        "    ind_test = permutation[:n_test]\n",
        "    \n",
        "    #If it is n_samples, it will be an error 0 ~ 99\n",
        "    ind_train = permutation[n_test:(n_test + n_train)]\n",
        "    \n",
        "    #X[[2,5,86,45,0]]←The line below looks like this\n",
        "    X_train = X[ind_train]\n",
        "    X_test = X[ind_test]\n",
        "    y_train = y[ind_train]\n",
        "    y_test = y[ind_test]\n",
        "    \n",
        "    return X_train, X_test, y_train, y_test"
      ],
      "metadata": {
        "id": "B6raOJ1Yzavy"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Implementation of code for machine learning using scikit-learn\n",
        "\n",
        "We will implement the code using scikit-learn to create a baseline model.\n",
        "\n",
        "Use the self-made function created in question 1 to divide the verification data. The hold out method may be used instead of cross Validation.\n",
        "\n",
        "Classification problem\n",
        "\n",
        "The classification is implemented using scikit-learn for three different methods.\n",
        "\n",
        "    Logistic regression\n",
        "    SVM\n",
        "    Decision tree\n",
        "\n",
        "\n",
        "It can be used in scikit-learn from two types, LogisticRegression class and SGDClassifier class. Here, use the SGDClassifier class that calculates using the gradient descent method. Logistic regression can be calculated by setting loss = ”log” as an argument.\n",
        "\n",
        "The scikit-learn as a classifier that can be used in logistic regression LogisticRegression class and SGDClassifier have classes are available. SGDClassifier class, which is calculated using the gradient descent method. Logistic regression can be calculated by specifying loss=\"log\" as an argument.\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier\n",
        "\n"
      ],
      "metadata": {
        "id": "BTWUnWO_oCjC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the operation using three types of data sets.\n",
        "\n",
        "The first is the iris dataset similar to the pre-learning period.\n",
        "\n",
        "sklearn.datasets.load_iris - scikit-learn stable version documentation:\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html\n",
        "\n",
        "Since we want to make a binary classification, we will use only the following two objective variables. All four types of features are used.\n",
        "\n",
        "    virgicolor and virginica\n",
        "\n",
        "\n",
        "The other two are artificial datasets with two feature values. With the following code we can create the explanatory variableXand the objective variable y. Let's call them \"Simple Data Set 1\" and \"Simple Data Set 2\". As there are only two features, visualization is easy.\n",
        "\n",
        "Simple data set 1 creation code"
      ],
      "metadata": {
        "id": "g0EbqsOPocTT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "np.random.seed(seed=0)\n",
        "n_samples = 500\n",
        "f0 = [-1, 2]\n",
        "f1 = [2, -1]\n",
        "cov = [[1.0,0.8], [0.8, 1.0]]\n",
        "f0 = np.random.multivariate_normal(f0, cov, n_samples // 2)\n",
        "f1 = np.random.multivariate_normal(f1, cov, n_samples // 2)\n",
        "X = np.concatenate([f0, f1])\n",
        "y = np.concatenate([\n",
        "    np.full(n_samples // 2, 1),\n",
        "    np.full(n_samples // 2, -1)\n",
        "])"
      ],
      "metadata": {
        "id": "_yyt3Z14nqf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Simple data set 2 creation code**"
      ],
      "metadata": {
        "id": "9JdfBW0iotIQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array([\n",
        "    [-0.44699 , -2.8073  ],[-1.4621  , -2.4586  ],\n",
        "    [ 0.10645 ,  1.9242  ],[-3.5944  , -4.0112  ],\n",
        "    [-0.9888  ,  4.5718  ],[-3.1625  , -3.9606  ],\n",
        "    [ 0.56421 ,  0.72888 ],[-0.60216 ,  8.4636  ],\n",
        "    [-0.61251 , -0.75345 ],[-0.73535 , -2.2718  ],\n",
        "    [-0.80647 , -2.2135  ],[ 0.86291 ,  2.3946  ],\n",
        "    [-3.1108  ,  0.15394 ],[-2.9362  ,  2.5462  ],\n",
        "    [-0.57242 , -2.9915  ],[ 1.4771  ,  3.4896  ],\n",
        "    [ 0.58619 ,  0.37158 ],[ 0.6017  ,  4.3439  ],\n",
        "    [-2.1086  ,  8.3428  ],[-4.1013  , -4.353   ],\n",
        "    [-1.9948  , -1.3927  ],[ 0.35084 , -0.031994],\n",
        "    [ 0.96765 ,  7.8929  ],[-1.281   , 15.6824  ],\n",
        "    [ 0.96765 , 10.083   ],[ 1.3763  ,  1.3347  ],\n",
        "    [-2.234   , -2.5323  ],[-2.9452  , -1.8219  ],\n",
        "    [ 0.14654 , -0.28733 ],[ 0.5461  ,  5.8245  ],\n",
        "    [-0.65259 ,  9.3444  ],[ 0.59912 ,  5.3524  ],\n",
        "    [ 0.50214 , -0.31818 ],[-3.0603  , -3.6461  ],\n",
        "    [-6.6797  ,  0.67661 ],[-2.353   , -0.72261 ],\n",
        "    [ 1.1319  ,  2.4023  ],[-0.12243 ,  9.0162  ],\n",
        "    [-2.5677  , 13.1779  ],[ 0.057313,  5.4681  ],\n",
        "])\n",
        "y = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
        "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ],
      "metadata": {
        "id": "GP0m_9p8om9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[Problem 2] Creating a code to solve the classification problem**\n",
        "\n",
        "Create code to train and estimate 3 types of datasets using the 3 types of methods above.\n",
        "\n",
        "Regression problem\n",
        "\n",
        "Next, we will implement one type of regression using scikit-learn.\n",
        "\n",
        "    Linear regression\n",
        "\n",
        "\n",
        "For linear regression, use SGDRegressor class, which is calculated using gradient descent.\n",
        "\n",
        "sklearn.linear_model.SGDRegressor - scikit-lear stable version documentation\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html\n",
        "\n",
        "The data set is from the House Prices competition as in the pre-study period.\n",
        "\n",
        "House Prices: Advanced Regression Techniques\n",
        "https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data\n",
        "\n",
        "Downloadtrain.csvand use SalePriceas the objective variable andGrLivAreaand YearBuiltas the explanatory variables."
      ],
      "metadata": {
        "id": "GluOOiRGpALg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Logistic regression**"
      ],
      "metadata": {
        "id": "goHw16HY1CsM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "iris = load_iris()\n",
        "\n",
        "X = iris.data[50:, :3]\n",
        "y = iris.target[50:]\n",
        "\n",
        "X_train, X_test, y_train, y_test = scratch_train_test_split(X, y, test_size =0.3, random_state=0)\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "sc = StandardScaler()\n",
        "X_std = sc.fit_transform(X_train)\n",
        "\n",
        "#検証用のデータにtransformを行います。\n",
        "X_test_std = sc.transform(X_test)\n",
        "\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "# sgdc = make_pipeline(StandardScaler(),SGDClassifier(loss ='log'))\n",
        "# sgdc.fit(X_train, y_train)\n",
        "# Y_pred = sgdc.predict(X_test)\n",
        "\n",
        "sgdc = SGDClassifier(loss='log')\n",
        "sgdc.fit(X_std, y_train)\n",
        "Y_pred = sgdc.predict(X_test_std)\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "\n",
        "print('accuracy = {}'.format(accuracy_score(y_test, Y_pred)))\n",
        "print('precision = {}'.format(precision_score(y_test, Y_pred)))\n",
        "print('recall = {}'.format(recall_score(y_test, Y_pred)))\n",
        "print('f1 score = {}'.format(f1_score(y_test, Y_pred)))\n",
        "print('confusion matrix = \\n{}'.format(confusion_matrix(y_test, Y_pred)))"
      ],
      "metadata": {
        "id": "WSTLyDIEowJE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b9256fc-e5c0-430b-fd23-5a45a6133e8d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy = 0.9\n",
            "precision = 0.9285714285714286\n",
            "recall = 0.8666666666666667\n",
            "f1 score = 0.896551724137931\n",
            "confusion matrix = \n",
            "[[13  2]\n",
            " [ 1 14]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SVM**"
      ],
      "metadata": {
        "id": "fGYAT8GU1E0b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "np.random.seed(seed=0)\n",
        "n_samples = 500\n",
        "f0 = [-1, 2]\n",
        "f1 = [2, -1]\n",
        "cov = [[1.0,0.8], [0.8, 1.0]]\n",
        "f0 = np.random.multivariate_normal(f0, cov, int(n_samples/2))\n",
        "f1 = np.random.multivariate_normal(f1, cov, int(n_samples/2))\n",
        "X = np.concatenate((f0, f1))\n",
        "y = np.concatenate((np.ones((int(n_samples/2))), np.ones((int(n_samples/2))) *(-1))).astype(np.int)\n",
        "random_index = np.random.permutation(np.arange(n_samples))\n",
        "X = X[random_index]\n",
        "y = y[random_index]\n",
        "\n",
        "X_train, X_test, y_train, y_test = scratch_train_test_split(X, y, test_size =0.3, random_state=0)\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "svm = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
        "svm.fit(X_train, y_train)\n",
        "Y_pred = svm.predict(X_test)\n",
        "\n",
        "print('accuracy = {}'.format(accuracy_score(y_test, Y_pred)))\n",
        "print('precision = {}'.format(precision_score(y_test, Y_pred)))\n",
        "print('recall = {}'.format(recall_score(y_test, Y_pred)))\n",
        "print('f1 score = {}'.format(f1_score(y_test, Y_pred)))\n",
        "print('confusion matrix = \\n{}'.format(confusion_matrix(y_test, Y_pred)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xrzJZLVQ1JUB",
        "outputId": "87e5e9a4-a91e-4ea3-ef2a-1c07689b5342"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy = 1.0\n",
            "precision = 1.0\n",
            "recall = 1.0\n",
            "f1 score = 1.0\n",
            "confusion matrix = \n",
            "[[65  0]\n",
            " [ 0 85]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Decision tree**"
      ],
      "metadata": {
        "id": "4H9aN6gw1Va5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array([[-0.44699 , -2.8073  ],[-1.4621  , -2.4586  ],\n",
        "       [ 0.10645 ,  1.9242  ],[-3.5944  , -4.0112  ],\n",
        "       [-0.9888  ,  4.5718  ],[-3.1625  , -3.9606  ],\n",
        "       [ 0.56421 ,  0.72888 ],[-0.60216 ,  8.4636  ],\n",
        "       [-0.61251 , -0.75345 ],[-0.73535 , -2.2718  ],\n",
        "       [-0.80647 , -2.2135  ],[ 0.86291 ,  2.3946  ],\n",
        "       [-3.1108  ,  0.15394 ],[-2.9362  ,  2.5462  ],\n",
        "       [-0.57242 , -2.9915  ],[ 1.4771  ,  3.4896  ],\n",
        "       [ 0.58619 ,  0.37158 ],[ 0.6017  ,  4.3439  ],\n",
        "       [-2.1086  ,  8.3428  ],[-4.1013  , -4.353   ],\n",
        "       [-1.9948  , -1.3927  ],[ 0.35084 , -0.031994],\n",
        "       [ 0.96765 ,  7.8929  ],[-1.281   , 15.6824  ],\n",
        "       [ 0.96765 , 10.083   ],[ 1.3763  ,  1.3347  ],\n",
        "       [-2.234   , -2.5323  ],[-2.9452  , -1.8219  ],\n",
        "       [ 0.14654 , -0.28733 ],[ 0.5461  ,  5.8245  ],\n",
        "       [-0.65259 ,  9.3444  ],[ 0.59912 ,  5.3524  ],\n",
        "       [ 0.50214 , -0.31818 ],[-3.0603  , -3.6461  ],\n",
        "       [-6.6797  ,  0.67661 ],[-2.353   , -0.72261 ],\n",
        "       [ 1.1319  ,  2.4023  ],[-0.12243 ,  9.0162  ],\n",
        "       [-2.5677  , 13.1779  ],[ 0.057313,  5.4681  ]])\n",
        "y = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
        "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
        "\n",
        "X_train, X_test, y_train, y_test = scratch_train_test_split(X, y, test_size =0.3, random_state=0)\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "clf = DecisionTreeClassifier(random_state=0)\n",
        "\n",
        "clf = DecisionTreeClassifier()\n",
        "clf.fit(X_train, y_train)\n",
        "Y_pred = clf.predict(X_test)\n",
        "\n",
        "print('accuracy = {}'.format(accuracy_score(y_test, Y_pred)))\n",
        "print('precision = {}'.format(precision_score(y_test, Y_pred)))\n",
        "print('recall = {}'.format(recall_score(y_test, Y_pred)))\n",
        "print('f1 score = {}'.format(f1_score(y_test, Y_pred)))\n",
        "print('confusion matrix = \\n{}'.format(confusion_matrix(y_test, Y_pred)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6rrGuFj1WM3",
        "outputId": "26db6901-07ff-4519-9a25-d7987572feba"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy = 0.8333333333333334\n",
            "precision = 0.8571428571428571\n",
            "recall = 0.8571428571428571\n",
            "f1 score = 0.8571428571428571\n",
            "confusion matrix = \n",
            "[[4 1]\n",
            " [1 6]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[Problem 3] Creating a code to solve the regression problem**\n",
        "\n",
        "Create code to train and estimate the House Prices data set with linear regression."
      ],
      "metadata": {
        "id": "0vUSq1D2pQJZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = pd.read_csv('/content/drive/MyDrive/my_sample_data/HousePricesData/train.csv')\n",
        "\n",
        "df = df_train.loc[:, ['GrLivArea','YearBuilt', 'SalePrice']]\n",
        "\n",
        "df1 = np.log(df)"
      ],
      "metadata": {
        "id": "JZDDUrJlpR4X"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Linear regression**"
      ],
      "metadata": {
        "id": "YoQwqJ9W10Qs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = scratch_train_test_split(np.array(df1.iloc[:, [0, 1]]), np.array(df1.iloc[:, [2]]), random_state = 0)\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "lr = LinearRegression()\n",
        "\n",
        "#Training with linear regression\n",
        "lr.fit(X_train, y_train)\n",
        "\n",
        "#Predicting\n",
        "Y_pred = lr.predict(X_test)\n",
        "\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "print ('mean absolute error (MAE) = {}'.format(mean_absolute_error(y_test, Y_pred)))\n",
        "print ('Mean squared error (MSE) = {}'.format(mean_squared_error(y_test, Y_pred)))\n",
        "print ('Root mean square error (RMSE) = {}'.format(np.sqrt(mean_squared_error (y_test, Y_pred))))\n",
        "print ('coefficient of determination (R2) = {}'.format(r2_score(y_test, Y_pred)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ku4fqUQv11KH",
        "outputId": "d217628c-d133-4389-ec7e-e199bfd7d539"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean absolute error (MAE) = 0.15353180375096695\n",
            "Mean squared error (MSE) = 0.04439252334052215\n",
            "Root mean square error (RMSE) = 0.2106953329822997\n",
            "coefficient of determination (R2) = 0.7123462478163505\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "XUPBlUoN5Co3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}