{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "diver_class_assignment8_ML_preclass_gradient_problem.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOoVU3ZMF2wHHkssSf9dLCD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Enkhzol/diveintocode-ml/blob/master/diver_class_assignment8_ML_preclass_gradient_problem.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkVwQHQdzuD_"
      },
      "source": [
        "**[Problem 1] Linear function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wtQVmWLSzyWm",
        "outputId": "d05cfd1d-e580-401a-da7b-f9c441841a4c"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "X = np.arange(-50,51)\n",
        "print(X)\n",
        "\n",
        "y = []\n",
        "for v in X:\n",
        "  y.append(0.5 * v + 1)\n",
        "\n",
        "Y = np.array(y)\n",
        "\n",
        "print(Y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-50 -49 -48 -47 -46 -45 -44 -43 -42 -41 -40 -39 -38 -37 -36 -35 -34 -33\n",
            " -32 -31 -30 -29 -28 -27 -26 -25 -24 -23 -22 -21 -20 -19 -18 -17 -16 -15\n",
            " -14 -13 -12 -11 -10  -9  -8  -7  -6  -5  -4  -3  -2  -1   0   1   2   3\n",
            "   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19  20  21\n",
            "  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39\n",
            "  40  41  42  43  44  45  46  47  48  49  50]\n",
            "[-24.  -23.5 -23.  -22.5 -22.  -21.5 -21.  -20.5 -20.  -19.5 -19.  -18.5\n",
            " -18.  -17.5 -17.  -16.5 -16.  -15.5 -15.  -14.5 -14.  -13.5 -13.  -12.5\n",
            " -12.  -11.5 -11.  -10.5 -10.   -9.5  -9.   -8.5  -8.   -7.5  -7.   -6.5\n",
            "  -6.   -5.5  -5.   -4.5  -4.   -3.5  -3.   -2.5  -2.   -1.5  -1.   -0.5\n",
            "   0.    0.5   1.    1.5   2.    2.5   3.    3.5   4.    4.5   5.    5.5\n",
            "   6.    6.5   7.    7.5   8.    8.5   9.    9.5  10.   10.5  11.   11.5\n",
            "  12.   12.5  13.   13.5  14.   14.5  15.   15.5  16.   16.5  17.   17.5\n",
            "  18.   18.5  19.   19.5  20.   20.5  21.   21.5  22.   22.5  23.   23.5\n",
            "  24.   24.5  25.   25.5  26. ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUtS9VI109Te"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGdwjluG2FPp"
      },
      "source": [
        "**[Problem 2] Array combination**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZuaWY7t2IOD",
        "outputId": "7e3fd302-3e6d-454b-8ec8-3838fb399afc"
      },
      "source": [
        "nd = np.concatenate((X, Y))\n",
        "#print(nd)\n",
        "\n",
        "nd = np.reshape(nd, (101, 2), order='F')\n",
        "print(nd)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-50.  -24. ]\n",
            " [-49.  -23.5]\n",
            " [-48.  -23. ]\n",
            " [-47.  -22.5]\n",
            " [-46.  -22. ]\n",
            " [-45.  -21.5]\n",
            " [-44.  -21. ]\n",
            " [-43.  -20.5]\n",
            " [-42.  -20. ]\n",
            " [-41.  -19.5]\n",
            " [-40.  -19. ]\n",
            " [-39.  -18.5]\n",
            " [-38.  -18. ]\n",
            " [-37.  -17.5]\n",
            " [-36.  -17. ]\n",
            " [-35.  -16.5]\n",
            " [-34.  -16. ]\n",
            " [-33.  -15.5]\n",
            " [-32.  -15. ]\n",
            " [-31.  -14.5]\n",
            " [-30.  -14. ]\n",
            " [-29.  -13.5]\n",
            " [-28.  -13. ]\n",
            " [-27.  -12.5]\n",
            " [-26.  -12. ]\n",
            " [-25.  -11.5]\n",
            " [-24.  -11. ]\n",
            " [-23.  -10.5]\n",
            " [-22.  -10. ]\n",
            " [-21.   -9.5]\n",
            " [-20.   -9. ]\n",
            " [-19.   -8.5]\n",
            " [-18.   -8. ]\n",
            " [-17.   -7.5]\n",
            " [-16.   -7. ]\n",
            " [-15.   -6.5]\n",
            " [-14.   -6. ]\n",
            " [-13.   -5.5]\n",
            " [-12.   -5. ]\n",
            " [-11.   -4.5]\n",
            " [-10.   -4. ]\n",
            " [ -9.   -3.5]\n",
            " [ -8.   -3. ]\n",
            " [ -7.   -2.5]\n",
            " [ -6.   -2. ]\n",
            " [ -5.   -1.5]\n",
            " [ -4.   -1. ]\n",
            " [ -3.   -0.5]\n",
            " [ -2.    0. ]\n",
            " [ -1.    0.5]\n",
            " [  0.    1. ]\n",
            " [  1.    1.5]\n",
            " [  2.    2. ]\n",
            " [  3.    2.5]\n",
            " [  4.    3. ]\n",
            " [  5.    3.5]\n",
            " [  6.    4. ]\n",
            " [  7.    4.5]\n",
            " [  8.    5. ]\n",
            " [  9.    5.5]\n",
            " [ 10.    6. ]\n",
            " [ 11.    6.5]\n",
            " [ 12.    7. ]\n",
            " [ 13.    7.5]\n",
            " [ 14.    8. ]\n",
            " [ 15.    8.5]\n",
            " [ 16.    9. ]\n",
            " [ 17.    9.5]\n",
            " [ 18.   10. ]\n",
            " [ 19.   10.5]\n",
            " [ 20.   11. ]\n",
            " [ 21.   11.5]\n",
            " [ 22.   12. ]\n",
            " [ 23.   12.5]\n",
            " [ 24.   13. ]\n",
            " [ 25.   13.5]\n",
            " [ 26.   14. ]\n",
            " [ 27.   14.5]\n",
            " [ 28.   15. ]\n",
            " [ 29.   15.5]\n",
            " [ 30.   16. ]\n",
            " [ 31.   16.5]\n",
            " [ 32.   17. ]\n",
            " [ 33.   17.5]\n",
            " [ 34.   18. ]\n",
            " [ 35.   18.5]\n",
            " [ 36.   19. ]\n",
            " [ 37.   19.5]\n",
            " [ 38.   20. ]\n",
            " [ 39.   20.5]\n",
            " [ 40.   21. ]\n",
            " [ 41.   21.5]\n",
            " [ 42.   22. ]\n",
            " [ 43.   22.5]\n",
            " [ 44.   23. ]\n",
            " [ 45.   23.5]\n",
            " [ 46.   24. ]\n",
            " [ 47.   24.5]\n",
            " [ 48.   25. ]\n",
            " [ 49.   25.5]\n",
            " [ 50.   26. ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iES-xXKu86Oa"
      },
      "source": [
        "**[Problem 3] Find the gradient**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wiy811D788bE",
        "outputId": "8da71013-8421-46b5-e976-65fd798a5f11"
      },
      "source": [
        "g = []\n",
        "\n",
        "for i in range(len(nd) - 1):\n",
        "  dx = nd[i + 1][0] - nd[i][0] \n",
        "  dy = nd[i + 1][1] - nd[i][1]\n",
        "  g.append(dy / dx)\n",
        "\n",
        "grad = np.array(g)\n",
        "print(grad)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
            " 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
            " 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
            " 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
            " 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
            " 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJrpzoBz-0b-"
      },
      "source": [
        "**[Problem 4] Draw a graph**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "BxnTIcBV-1ex",
        "outputId": "b3b8ad16-1687-4fbe-b773-151fb4e9f3f5"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x = X[0:-1]\n",
        "y = grad\n",
        "\n",
        "print(len(x))\n",
        "print(len(y))\n",
        "\n",
        "plt.plot(x, y, '-r', label='gradient')\n",
        "plt.title('Graph of gradient')\n",
        "plt.xlabel('x', color='#1C2833')\n",
        "plt.ylabel('y', color='#1C2833')\n",
        "plt.legend(loc='upper left')\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100\n",
            "100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wbmj1pq9AmRy"
      },
      "source": [
        "**[Problem 5] Python functionalization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vCFRvC9BBGXL",
        "outputId": "c2590051-57cc-446f-8be2-efab54be8839"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\"\"\"\n",
        "    Calculate the gradient using the amount of change.\n",
        "    Parameters\n",
        "    ----------------\n",
        "    function : function\n",
        "      The function you want to find, the one that returns the ndarray of y when you put the ndarray of x.\n",
        "    x_range : tuple\n",
        "      Specify the range in the same way as the argument of np.arange ().\n",
        "    Returns\n",
        "    ----------------\n",
        "    array_xy : ndarray, shape(n, 2)\n",
        "      A combination of x and y. n depends on x_range.\n",
        "    gradient : ndarray, shape(n-1,)\n",
        "      Function gradient. One factor is reduced to make a difference\n",
        "    \"\"\"\n",
        "def compute_gradient(function, x_range=(-50, 50.1, 0.1)):\n",
        "  array_y = function(x_range)\n",
        "  nd = np.concatenate((x_range, array_y))\n",
        "  array_xy = np.reshape(nd, (len(x_range), 2), order='F')\n",
        "\n",
        "  g = []\n",
        "\n",
        "  for i in range(len(array_xy) - 1):\n",
        "    dx = array_xy[i + 1][0] - array_xy[i][0] \n",
        "    dy = array_xy[i + 1][1] - array_xy[i][1]\n",
        "    g.append(dy / dx)\n",
        "\n",
        "  gradient = np.array(g)\n",
        "\n",
        "  pass\n",
        "  return array_xy, gradient\n",
        "\n",
        "def function1(x_range):\n",
        "  y = []\n",
        "  for v in x_range:\n",
        "    y.append(v * v)\n",
        "\n",
        "  array_y = np.array(y)\n",
        "  pass\n",
        "  return array_y\n",
        "\n",
        "def function2(x_range):\n",
        "  y = []\n",
        "  for v in x_range:\n",
        "    y.append(2 * v * v + 2**v)\n",
        "\n",
        "  array_y = np.array(y)\n",
        "  pass\n",
        "  return array_y\n",
        "\n",
        "def function3(x_range):\n",
        "  y = []\n",
        "  for v in x_range:\n",
        "    y.append(np.sin(v**0.5))\n",
        "\n",
        "  array_y = np.array(y)\n",
        "  pass\n",
        "  return array_y\n",
        "\n",
        "print(\"---------- function1 ----------\")\n",
        "array_xy, gradient = compute_gradient(function1)\n",
        "print(\"array_xy = \", array_xy)\n",
        "print(\"gradient = \",gradient)\n",
        "\n",
        "print(\"---------- function2 ----------\")\n",
        "array_xy, gradient = compute_gradient(function2)\n",
        "print(\"array_xy = \", array_xy)\n",
        "print(\"gradient = \",gradient)\n",
        "\n",
        "print(\"---------- function3 ----------\")\n",
        "array_xy, gradient = compute_gradient(function3)\n",
        "print(\"array_xy = \", array_xy)\n",
        "print(\"gradient = \",gradient)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------- function1 ----------\n",
            "array_xy =  [[-5.00000e+01  2.50000e+03]\n",
            " [ 5.01000e+01  2.51001e+03]\n",
            " [ 1.00000e-01  1.00000e-02]]\n",
            "gradient =  [ 0.1 50.2]\n",
            "---------- function2 ----------\n",
            "array_xy =  [[-5.00000000e+01  5.00000000e+03]\n",
            " [ 5.01000000e+01  1.20670964e+15]\n",
            " [ 1.00000000e-01  1.09177346e+00]]\n",
            "gradient =  [1.20550414e+13 2.41341928e+13]\n",
            "---------- function3 ----------\n",
            "array_xy =  [[-5.00000000e+01  +0.j          2.54895347e-13+588.70188026j]\n",
            " [ 5.01000000e+01  +0.j          7.13828618e-01  +0.j        ]\n",
            " [ 1.00000000e-01  +0.j          3.10983593e-01  +0.j        ]]\n",
            "gradient =  [0.00713116-5.88113766j 0.0080569 -0.j        ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhEyckFSMjqA"
      },
      "source": [
        "**[Problem 6] Find the minimum value**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X3sqWZbLMmMN",
        "outputId": "cc2be013-a247-4bcd-d0df-41ac78099197"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\"\"\"\n",
        "    Calculate the gradient using the amount of change.\n",
        "    Parameters\n",
        "    ----------------\n",
        "    function : function\n",
        "      The function you want to find, the one that returns the ndarray of y when you put the ndarray of x.\n",
        "    x_range : tuple\n",
        "      Specify the range in the same way as the argument of np.arange ().\n",
        "    Returns\n",
        "    ----------------\n",
        "    array_xy : ndarray, shape(n, 2)\n",
        "      A combination of x and y. n depends on x_range.\n",
        "    gradient : ndarray, shape(n-1,)\n",
        "      Function gradient. One factor is reduced to make a difference\n",
        "    \"\"\"\n",
        "def compute_gradient(function, x_range=(-50, 50.1, 0.1)):\n",
        "  array_y = function(x_range)\n",
        "\n",
        "  print(\"array_y.min = \", array_y.min(axis=0))\n",
        "  print(\"array_y.argmin = \", array_y.argmin(axis=0))\n",
        "\n",
        "  nd = np.concatenate((x_range, array_y))\n",
        "  array_xy = np.reshape(nd, (len(x_range), 2), order='F')\n",
        "\n",
        "  g = []\n",
        "\n",
        "  for i in range(len(array_xy) - 1):\n",
        "    dx = array_xy[i + 1][0] - array_xy[i][0] \n",
        "    dy = array_xy[i + 1][1] - array_xy[i][1]\n",
        "    g.append(dy / dx)\n",
        "\n",
        "  gradient = np.array(g)\n",
        "\n",
        "  pass\n",
        "  return array_xy, gradient\n",
        "\n",
        "def function1(x_range):\n",
        "  y = []\n",
        "  for v in x_range:\n",
        "    y.append(v * v)\n",
        "\n",
        "  array_y = np.array(y)\n",
        "  pass\n",
        "  return array_y\n",
        "\n",
        "def function2(x_range):\n",
        "  y = []\n",
        "  for v in x_range:\n",
        "    y.append(2 * v * v + 2**v)\n",
        "\n",
        "  array_y = np.array(y)\n",
        "  pass\n",
        "  return array_y\n",
        "\n",
        "def function3(x_range):\n",
        "  y = []\n",
        "  for v in x_range:\n",
        "    y.append(np.sin(v**0.5))\n",
        "\n",
        "  array_y = np.array(y)\n",
        "  pass\n",
        "  return array_y\n",
        "\n",
        "print(\"---------- function1 ----------\")\n",
        "array_xy, gradient = compute_gradient(function1)\n",
        "print(\"array_xy = \", array_xy)\n",
        "print(\"gradient = \",gradient)\n",
        "\n",
        "print(\"---------- function2 ----------\")\n",
        "array_xy, gradient = compute_gradient(function2)\n",
        "print(\"array_xy = \", array_xy)\n",
        "print(\"gradient = \",gradient)\n",
        "\n",
        "print(\"---------- function3 ----------\")\n",
        "array_xy, gradient = compute_gradient(function3)\n",
        "print(\"array_xy = \", array_xy)\n",
        "print(\"gradient = \",gradient)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------- function1 ----------\n",
            "array_y.min =  0.010000000000000002\n",
            "array_y.argmin =  2\n",
            "array_xy =  [[-5.00000e+01  2.50000e+03]\n",
            " [ 5.01000e+01  2.51001e+03]\n",
            " [ 1.00000e-01  1.00000e-02]]\n",
            "gradient =  [ 0.1 50.2]\n",
            "---------- function2 ----------\n",
            "array_y.min =  1.0917734625362931\n",
            "array_y.argmin =  2\n",
            "array_xy =  [[-5.00000000e+01  5.00000000e+03]\n",
            " [ 5.01000000e+01  1.20670964e+15]\n",
            " [ 1.00000000e-01  1.09177346e+00]]\n",
            "gradient =  [1.20550414e+13 2.41341928e+13]\n",
            "---------- function3 ----------\n",
            "array_y.min =  (2.548953470043686e-13+588.7018802618827j)\n",
            "array_y.argmin =  0\n",
            "array_xy =  [[-5.00000000e+01  +0.j          2.54895347e-13+588.70188026j]\n",
            " [ 5.01000000e+01  +0.j          7.13828618e-01  +0.j        ]\n",
            " [ 1.00000000e-01  +0.j          3.10983593e-01  +0.j        ]]\n",
            "gradient =  [0.00713116-5.88113766j 0.0080569 -0.j        ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHVA-3yg2yC3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}